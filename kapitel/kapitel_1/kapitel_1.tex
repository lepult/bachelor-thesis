\newpage
\section{Grundlagen}\label{Grundlagen}
Fehlender-Text

\subsection{Service Roboter}
In den letzten Jahren haben Service Roboter in verschiedenen Branchen an Bedeutung gewonnen. Ein Grund hierfür ist der technische Fortschritt in Robotik, KI, Big Data, Kameras, Sensoren und Spracherkennung \cite[S.~424]{Paluch2020}. Dieser Abschnitt gibt einen kurzen Überblick über Service Roboter im allgemeinen und eine Einführung in die Funktionen der Roboter, die in dieser Arbeit eingesetzt werden. Auch wird das \ac{BCB} genauer erläutert.

\subsubsection{Definition}
In wissenschaftlichen Arbeiten wird mit vielen verschiedenen Definitionen für Service Roboter gearbeitet. In dieser Arbeit wird die Definition aus der ISO Norm 8373:2021 \cite[Kap.~3]{ISO2021} verwendet. Nach dieser handelt es sich bei Service Robotern um Roboter die im privaten oder professionellen Gebrauch nützliche Aufgaben für Menschen erledigen. Service Roboter werden von Industrierobotern und Medizinrobotern abgegrenzt. Die \ac{IFR} \cite{IFR2024} ergänzt die Voraussetzung, dass Service Roboter voll- oder zumindest teilautonom handeln können. Unter professionellem Gebrauch versteht man den kommerziellen Einsatz \cite[S.~4]{GonzalezAguirre2021}, unter anderem im Gesundheitswesen, in der Landwirtschaft und im Tourismus \cite[S.~9]{GonzalezAguirre2021}.

\subsubsection{Einsatzmöglichkeiten}
Service Roboter werden bereits in vielen Bereichen eingesetzt. So gibt es verschiedene Beispiele in denen Service Roboter in Hotels für Gästeempfang, Check-in und Gepäcklieferung und an Flughäfen für die Beratung von Reisenden, Scannen von Boardingpässen, Check-in, Bodenreinigung und Patrouilliengänge genutzt werden. In der Pflege helfen Service Roboter den Pflegern beim Heben von Patienten und beim Durchführen von Übungen mit Patientengruppen.\cite[S.~425-427]{Paluch2020} Aufgaben mit geringer kognitiver und emotionaler Komplexität können hierbei vollautonom und ohne Aufsicht durch einen Menschen durchgeführt werden \cite[S.~429]{Paluch2020}. Unter solche Aufgaben fällt zum Beispiel Staubsaugen, Rasenmähen oder Gepäcklieferung. Für komplexere Aufgaben braucht es die Aufsicht oder Unterstützung von Menschen, wodurch diese nur teilautonom ausgeführt werden. Service Roboter, die im Kontakt mit Kunden eingesetzt werden, bieten verschiedene Vorteile, die aber immer abgewägt werden müssen. Beispielsweise können Roboter Emotionen vorspielen, die von Kunden aber als unauthentisch erkannt werden. Gleichzeitig können Roboter dafür aber im Gegensatz zu Menschen ununterbrochen freundlich sein \cite[S.~427]{Paluch2020}. 

\subsubsection{Pudu Robotics}
Wie erwähnt, beschäftigt sich diese Arbeit mit Robotern von Pudu. Pudu stellt Service Roboter her, die vor allem in der Gastronomie eingesetzt werden können. Die Modelle sind hierbei auf unterschiedliche Funktionen, wie das Begrüßen von Gästen, das Liefern bestellter Speisen und Getränke, das Zurückbringen dreckigen Geschirs und das Putzen des Bodens spezialisiert \cite{PUDU2024}. Damit die Roboter diese Funktionen ausführen können, müssen sie eigenständig durch komplexe, sich ändernde Umgebungen navigieren können. Das eigenständige Navigieren lässt sich in die Teilfunktionen Positionsfindung, Wahrnehmung und Routenplanung aufteilen, wobei die Positionsfindung eine Schlüsselrolle spielt \cite{Nature2022}. Zur Positionsfindung erstellen sich die Roboter mit \ac{VSLAM} eine Karte ihrer Umgebung, was bei einer Fläche von 1000 Quadratmetern eine Stunde dauern kann. Während Roboter normalerweise platzierte Markierungen brauchen, können sich die Pudu Roboter mithilfe einer nach oben gerichteten Kamera anhand der Zimmerdecke orientieren.\cite{Pudu2023} Durch weitere Kameras und Sensoren können die Pudu Roboter ihre Umgebung wahrnehmen\cite{Nature2022}.

\subsubsection{Bot Control Backend}\label{sec:BotControlBackend}
Im Rahmen dieser Arbeit wird das bereits existierende \ac{BCB} als Schnittstelle zwischen Robotern und Prototyp genutzt. In diesem Abschnitt wird die Kommunikation zwischen dem \ac{BCB} und den Pudu Robotern erläutert. Die folgenden Informationen zum Service Framework der Roboter stammen aus dem SDK Guidance Document \cite{PuduSDK}, das Tobit durch Pudu zur verfügung gestellt wurde.

Die Abbildung \ref{fig:BotControlBackendCommunication} veranschaulicht die Kommunikation zwischen dem \ac{BCB} und den Robotern. Wie man in der Abbildung sieht hat das \ac{BCB} nur eine direkte Verbindung zum Node.js Microservice, der wiederum über die PUDU Cloud mit den Robotern kommuniziert. Über \gls{HTTP}-Anfragen an den Microservice können Befehle gegeben und Daten abgefragt werden. Die Anfragen werden vom Microservice via \gls{MQTT} an die PUDU Cloud weitergeleitet. Anfragen die nicht an die Roboter weitergeleitet werden müssen, weil die angefragten Daten in der PUDU Cloud liegen, werden auf dem gleichen Weg beantwortet. Muss mit den Robotern kommuniziert werden, dann wird die Anfrage von der PUDU Cloud über \gls{MQTT} an die relevanten Roboter weitergeleitet, die diese dann beantworten. Die Roboter können auch unaufgefordert Ereignisse an das \ac{BCB} kommunizieren. Hierfür muss das \ac{BCB} eine Adresse für einzelne Ereignistypen im Microservice als \gls{Webhook} registrieren. Tritt das entsprechende Ereignis auf, schickt der Roboter diese Information via \gls{MQTT} über die PUDU Cloud an den Microservice. Dieser schickt daraufhin eine HTTP-Anfrage an die registrierte Adresse im \ac{BCB}.
% TODO Prüfen ob MQTT so funktioniert!

\begin{figure}[H]
\caption{Kommunikation zwischen Bot Control Backend und Robotern}\label{fig:BotControlBackendCommunication}
\includegraphics[width=0.9\textwidth]{BotControlBackend Diagramm}
\\
Quelle: In Anlehnung an Pudu \cite[S.~4]{PuduSDK}
\end{figure}

Das \ac{BCB} fungiert nicht nur als Schnittstelle zu den Robotern, sondern abstrahiert auch neue Funktionen aus den bereits vorhandenen. So können Roboter durch das \ac{BCB} Fahrstuhl fahren und somit Lieferpunkte in anderen Stockwerken erreichen. Ebenso können sie durch das \ac{BCB} an geschlossenen Tür halten, diese öffnen und anschließend weiterfahren. Es gibt verschiedene Daten, die für den Prototyp relevant sind und über das \ac{BCB} abgerufen werden können. Dazu gehören die Position der Roboter innerhalb ihrer internen Karte sowie die Positionen von wichtigen Standorten wie Lieferpunkten und Ladestationen. Zusätzlich können auch die Pfade angefragt werden, an denen sich die Roboter während der Fahrt orientieren. Darüber hinaus können virtuelle Wände angefragt werden, die manuell platziert werden müssen, um sicherzustellen, dass bestimmte Bereiche nicht durchfahren werden. Diese agieren dann aus der Sicht der Roboter wie echte Wände.

\subsection{Webanwendungen}
% Plattformunabhängigkeit mit Quelle erwähnen
Fehlender-Text

\subsubsection{Technologien und Softwarebibliotheken}\label{sec:WebTechnologies}
TODO: HTML erklären; Typescript und Javascript erklären; \ac{Sass} und CSS erklären; React erklären; React Redux erklären; Deployment erklären.

\subsubsection{chayns}\label{sec:Chayns}
TODO: chayns-Seite erläutern; chayns-Application erläutern; UAC-Gruppen erläutern; Admin-Modus erläutern; npx create-chayns-app; chayns-components; chayns-api; chayns.space erläutern; websocket-service erwähnen.

\subsection{3D Modelle}
Dieser Abschnitt bietet eine kurze Einführung in 3D-Modelle. Daraufhin werden verschiedene Methoden zur Erzeugung von 3D-Gebäudemodellen vorgestellt. Abschließend wird erläutert, wie 3D-Modelle in Webanwendungen eingebunden werden können.

3D-Modelle sind digitale Darstellungen von Objekten oder Szenen in drei Dimensionen. Anders als bei herkömmlichen 2D-Grafiken, die lediglich Breite und Höhe haben, enthalten 3D-Modelle zusätzlich Tiefeninformationen. Polygonale 3D-Modelle bestehen aus Polygonen, die sich aus Eckpunkten und Verbindungen zwischen Eckpunkten in der Form von Kanten zusammensetzen. Oberflächeneigenschaften der Polygone, wie Farbe, Glanz und Reflexionen lassen sich durch die Anwendung von Texturen und Materialien definieren. Transformationsoperationen wie Skalierung, Rotation und Translation ermöglichen es, 3D-Modelle im Raum zu bewegen und zu manipulieren. Kameras, Perspektiven und Projektionen den Betrachterstandpunkt und die Darstellung des Modells festlegen.\cite[S.~8-16]{Parisi2014}

\subsubsection{Generierung}
Neben der manuellen Modellierung von 3D-Modellen mithilfe von Modellierungssoftware wie Blender gibt es verschiedene Methoden, die sich insbesondere zur Generierung von Raummodellen eignen.

\paragraph{Fotogrammetrie}

Die Fotogrammetrie beschäftigt sich damit Messungen aus einer Vielzahl an zweidimensonalen Bildern abzuleiten und mit diesen präzise 3D-Modelle zu erzeugen\cite[S.~19]{Aber2010}. Inzwischen erfordert Fotogrammetrie nicht mehr den Einsatz teurer Kameras, da die Kameras moderner Mobilgeräte eine ausreichende Bildqualität bieten\cite{Cohrs2021}. Der Prozess der Fotogrammetrie lässt sich in mehrere Schritte gliedern. Zunächst braucht es eine sorgfältige Planung der Aufnahmen. Hierbei sollte auf eine gleichmäßige Belichtung geachtet werden. Außerdem sollten innerhalb der Szene reflektierende und transparenten Flächen sowie jegliche Bewegung von Objekten vermieden werden. Während der Aufnahme müssen Parameter wie Belichtungszeit und Weißabgleich passend konfiguriert sein, wobei diese Einstellungen zwischen den einzelnen Aufnahmen unverändert bleiben sollten. Des Weiteren ist darauf zu achten, dass sich der Inhalt aufeinanderfolgender Bilder stets überschneidet.\cite{Cohrs2021b} Zum Schluss erfolgt die Verarbeitung der Aufnahmen mithilfe spezialisierter Software, deren Bedienung komplex sein kann. Für eine reibungslose Verarbeitung sind eine leistungsstarke Grafikkarte und ausreichend Speicherplatz unerlässlich.\cite{Cohrs2021c}

\paragraph{LiDAR Scanning}
Im Gegensatz zur Fotogrammetrie nutzt \ac{LiDAR} einen aktiven Sensor. Dabei wird Licht in Form eines pulsierenden Lasers ausgesendet. Das reflektierte Licht wird darufhin mit einem Scanner erfasst, wodurch die Berechnung von Distanzen zu Punkten ermöglicht wird. Auf deren Grundlage wird dann ein 3D-Modell erstellt. Seit 2020 werden LiDAR-Scanner in neuere iOS-Geräte von Apple integriert, in der Hoffnung eine verbesserte Bildqualität zu ermöglichen\cite{Fenstermaker2022}. Als Nebeneffekt wurden verschiedene Apps entwickelt, die den LiDAR-Scanner nutzen, um 3D-Modelle zu erstellen, wie zum Beispiel Canvas\cite{Canvas2023}, Polycam\cite{Polycam2024} und Scaniverse\cite{Scaniverse2024}. Diese Apps versprechen eine einfache Generierung von 3D-Modellen.

\paragraph{KI gestützte Methoden}
Es existieren verschiedene KI-Modelle, die darauf trainiert sind, ein 3D-Gebäudemodell mit nur wenigen Bildern zu erzeugen. Eines dieser Modelle ist Plan2Scene. Es benötigt einen Grundriss des Gebäudes als Eingabe sowie Bilder, die den einzelnen Räumen zugeordnet sind. Basierend auf dem Grundriss generiert das Modell ein 3D-Modell mit Möbeln. Basierend auf den Bildern der Räume werden monotone Texturen für Wände und Böden generiert.\cite[S.~10733]{Plan2Scene2021} Das Rent3D Modell funktioniert ähnlich, generiert aber keine Texturen für Wände und Böden und nutzt stattdessen die Bildaufnahmen selbst als Textur\cite[S.~3413]{Rent3D2015}.

\subsubsection{Einbindung im Web}
Die Einbindung von 3D-Modellen wird im Web durch \ac{WebGL} und WebGPU ermöglicht. Während \ac{WebGL} für lange Zeit der etablierte Standard war, gewinnt WebGPU seit der Veröffentlichung im Jahr 2021 stetig an Popularität.

\paragraph{WebGL}
\ac{WebGL} wurde 2011 von der Khronos Group entwickelt und ist eine Javascript-API mit der 3D-Grafiken im Webbrowser ohne zusätzliche Plugins dargestellt werden können. Die 3D-Grafiken können hierbei hardwarebeschleunigt, also über den Einsatz spezialisierter Hardware - wie einer \ac{GPU} - angezeigt werden. Hierdurch wird eine hohe Leistungsfähigkeit ermöglicht, solange ein Gerät Hardwarebeschleunigung unterstützt. Durch die Integration mit HTML und Javascript können 3D-Grafiken dynamisch in Webseiten eingebunden werden. Da \ac{WebGL} auf offenen Webstandards basiert, ist es in allen Browsern plattformunabhängig sowohl an Desktop- als auch an Mobilgeräten nutzbar.\cite[S.~17-19]{Parisi2014} WebGPU bietet wie WebGL das hardwarebeschleunigte Anzeigen von 3D-Grafiken und darüber hinaus eine verbesserte Leistung sowie erweiterte Funktionen\cite{Surma2022}. Entwicklern steht eine Vielzahl an Frameworks zur Verfügung die auf \ac{WebGL} oder WebGPU basieren\cite{Seguin2024} und die Entwicklung im Vergleich zu diesen vereinfachen und beschleunigen.

\paragraph{\deckgl{}}
Das Framework \deckgl{} wurde 2016 von Uber als Open Source Projekt veröffentlicht\cite{Visgl}. Das Framework basiert auf \ac{WebGL}, wobei ab der kommenden Version 9.0.0 stattdessen WebGPU eingesetzt werden soll\cite{Green2022}. Mit \deckgl lassen sich hochperformante interaktive Karten und Geovisualisierungen mit tausenden bis millionen Datenpunkten im Web einbinden. Da das Framework auf React ähnlichen Programmierparadigmen basiert eignet es sich besonders gut für die Einbindung in React Anwendungen. Das Framework funktioniert nach dem \ac{PIL} Paradigma. So werden Datenpunkte gruppiert in einer Ebene dargestellt, die mindestens ein grundlegendes visuelles Element nutzt. So kann es sich bei diesen grundlegenden Elementen um Kreise, Rechtecke und Linien aber auch komplexere teilweise auch dreidimensonale Elemente handeln. Diese Elemente werden innerhalb der Ebene auf Basis der Datenpunkte und deren Attribute positioniert, skaliert und gefärbt. Die Ebenen können gestapelt und somit kombiniert werden, was auch die Inspiration für den Namen des Frameworks ist, da im Englischen deck of cards Kartenstapel bedeutet.\cite[S.~2]{YangWang2019} Das Framework bietet verschiedene vordefinierte Ebenen, wie die IconLayer \cite{DeckglIconLayer}, die Bilder als das grundlegende visuelle Element nutzt. So kann kann einem Datenpunkt ein Bild an der passenden Position zugewiesen werden. In dem Beispiel aus Abbildung \ref{fig:IconLayerExample} wird die IconLayer - in Kombination mit einer Basisebene zur Darstellung der Weltkarte - genutzt um die Positionen aller bekannten Meteoritenlandungen auf der Erde anzuzeigen.

\begin{figure}[H]
    \caption{IconLayer Beispiel}\label{fig:IconLayerExample}
    \includegraphics[width=0.9\textwidth]{IconLayer Example.png}
    \\
    Quelle: OpenJS Foundation \cite{DeckGlMeteorites}
\end{figure}

Weitere für diese Arbeit relevante vordefinierte Ebenen sind die SimpleMeshLayer \cite{DeckglSimpleMeshLayer} und ScenegraphLayer \cite{DeckglScenegraphLayer} für die Anzeige von 3D-Modellen und die PathLayer für das Anzeigen von Pfaden. Es können auch neue Ebenen entwickelt werden, was für diese Arbeit aber nicht relevant ist, da die Auswahl an vordefinierten Ebenen ausreicht. Weitere wichtige Elemente die \deckgl{} bietet sind die Controller Klasse \cite{DeckglController}, mit der die Navigation auf der Karte konfiguriert werden kann und die Viewport Klasse \cite{DeckglViewport} mit der die Navigation direkt gesteuert werden kann.

\subsection{Softwarequalität}
``Unter Softwarequalität versteht man die Gesamtheit der Merkmale und Merkmalswerte eines Softwareprodukts, die sich auf dessen Eignung beziehen, festgelegte oder vorausgesetzte Erfordernisse zu erfüllen'' \cite[S.~257]{Balzert1998}. So ergibt sich die Softwarequalität aus der Erfüllung der definierten Anforderungen und Erwartungen und zielt darauf ab den Bedürfnissen der Benutzer gerecht zu werden. Für die Bestimmung der Softwarequalität gibt es verschiedene Merkmale die ausgewertet werden können. Nach dem ISO-Standard ISO/IEC 25010 gibt es acht Produktqualitätsmerkmale \cite{ISO25010}, die auch in der Abbildung \ref{fig:SoftwareQuality} aufgelistet sind. Die Merkmale Effizienz und Benutzbarkeit haben Relevanz für diese Arbeit, da sie direkt in der Forschungsfrage gefordert werden. Auch die funktionale Eignung ist relevant, da diese die Erfüllung der funktionalen Anforderungen abdeckt, die in Kapitel \ref{sec:FunctionalRequirements} definiert werden. Diese drei Mermale sind in der Abbildung entsprechend gekennzeichnet. Da nur ein Prototyp entwickelt wird, der die Ansprüche an ein Produktivsystem nicht erfüllen muss, werden die restlichen Merkmale nicht weiter beachtet.

\begin{figure}[H]
    \caption{Qualitätsmerkmale}\label{fig:SoftwareQuality}
    \includegraphics[width=0.9\textwidth]{SoftwareQuality.png}
    \\
    Quelle: Eigene Darstellung
\end{figure}

\subsubsection{Benutzbarkeit}
Die Benutzbarkeit wird nach ISO 25010 in sechs Kriterien aufgeteilt: angemessene Erkennbarkeit, Erlernbarkeit, Bedienbarkeit, Toleranz gegenüber Anwenderfehlern, Ästhetik der Benutzeroberfläche und Barrierefreiheit.\cite{ISO25010} Mithilfe dieser Merkmale soll bewertet werden können wie einfach und angenehm ein Produkt für einen Nutzer zu bedienen ist.

\paragraph{Usability Heuristics}
Die zehn Usability Heuristics von Nielsen sind grundlegende Richtlinien zur Bewertung der Benutzerfreundlichkeit von Softwareprodukten und Websites. Während der Entwicklung können diesen Richtlinien berücksichtigt werden, um die Benutzerfreundlichkeit zu verbessern. Bis auf die Barrierefreiheit decken die Usability Heuristics alle Kriterien der Benutzbarkeit nach ISO 25010 ab. Zwei der Usability Heuristics sind die Übereinstimmung zwischen dem System und der realen Welt, um die Lernkurve zu reduzieren und die Einhaltung von Konsistenz und Standards, damit Elemente möglichst selbsterklärend sind.\cite{Nielsen.1994} Eine subjektive Bewertung der Einhaltung dieser Richtlinien durch die Entwickler ist unzureichend um die Benutzerfreundlichkeit zu bewerten. Stattdessen eignen sich hier Expertenreviews oder Usability Tests besser.

\paragraph{Usability Tests}
Für Usability Tests werden Testpersonen bei der Nutzung des zu testenden Produkts beobachtet. Die Testpersonen spielen hierbei zuvor entwickelte Anwendungsszenarien durch. Bei den Testpersonen sollte es sich um potenzielle Benutzer des Produkts handeln.\cite[S.~22]{Dumas.1999} Nach der Durchführung der Tests werden die gesammelten Beobachtungen auf Probleme und Schwachstellen im Produkt ausgewertet. Die Tests können entweder quanitativ oder qualitativ durchgeführt werden. Bei quantitativen Usability Tests werden verschiedene Metriken, wie die Durchführungszeit oder die Rate der erfolgreichen Durchführung von Aufgaben gesammelt. Diese Metriken zeigen im Vergleich zu den Ergebnissen früherer oder zukünftiger Tests, wie sich die Benutzerfreundlichkeit zwischen verschiedenen Versionen entwickelt hat. Bei qualitativen Usability Tests werden Testpersonen bei der Interaktion mit dem Produkt beobachtet, wodurch sich Designmerkmale identifizieren lassen, die gut oder schlecht zu bedienen sind.\cite{Budiu.2017}. Qualitative Tests brauchen einen Moderator, der die Testpersonen durch den Testprozess leitet. Gegebenenfalls gibt es auch weitere Beobachter, die nicht mit den Testpersonen interagieren.\cite{Moran.2019} Für qualitative Tests reicht eine Auswahl von fünf Testpersonen aus, um einen Großteil der Probleme zu finden. Mit einer zunehmenden Menge an Testpersonen sinkt das Return of Investment maßgeblich dadurch, dass immer weniger neue Fehler pro Testperson entdeckt werden.\cite{Nielsen.2012}

\subsubsection{Performance}

\subsubsection{Softwaretest}